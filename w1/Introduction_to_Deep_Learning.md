### 1. What does the analogy “AI is the new electricity” refer to?

- AI is powering personal devices in our homes and offices, similar to electricity.
- Through the “smart grid”, AI is delivering a new wave of electricity.
- :white_check_mark: Similar to electricity starting about 100 years ago, AI is transforming multiple industries.
- AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before.

### 2. Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.)

- Neural Networks are a brand new field.
- :white_check_mark:  We have access to a lot more data.
- :white_check_mark:  We have access to a lot more computational power.
- :white_check_mark:  Deep learning has resulted in significant improvements in important applications such as online
  advertising, speech recognition, and image recognition.

### 3. Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all

that apply.)

<img width="285" alt="Screenshot 2021-05-16 at 17 05 30" src="https://user-images.githubusercontent.com/38349049/118402213-3ad83000-b669-11eb-8487-6430630497d6.png"><br>

- It is faster to train on a big dataset than a small dataset.
- :white_check_mark: Recent progress in deep learning algorithms has allowed us to train good models faster (even
  without changing the CPU/GPU hardware). <br>
  *Yes. For example, we discussed how switching from sigmoid to ReLU activation functions allows faster training.

- :white_check_mark:  Faster computation can help speed up how long a team takes to iterate to a good idea.
- :white_check_mark:  Being able to try out ideas quickly allows deep learning engineers to iterate more quickly.

### 4. When an experienced deep learning engineer works on a new problem, they can usually use insight from previous

problems to train a good model on the first try, without needing to iterate multiple times through different models.
True/False?

- True
- :white_check_mark:  False

### 5. Which one of these plots represents a ReLU activation function? <br>

<img width="245" alt="Screenshot 2021-05-16 at 17 11 00" src="https://user-images.githubusercontent.com/38349049/118402320-b508b480-b669-11eb-879e-952cb783b961.png"><br>

- :white_check_mark:  Figure 3

### 6. Images for cat recognition is an example of “structured” data, because it is represented as a structured array

in a computer. True/False?

- True
- :white_check_mark:  False

### 7. A demographic dataset with statistics on different cities' population, GDP per capita, economic growth is an example of “unstructured” data because it contains data coming from different sources. True/False?

- True
- :white_check_mark:  False

### 8. Why is an RNN (Recurrent Neural Network) used for machine translation, say translating English to French? (Check all that apply.)

- RNNs represent the recurrent process of Idea->Code->Experiment->Idea->....
- :white_check_mark: It can be trained as a supervised learning problem.
- :white_check_mark: It is applicable when the input/output is a sequence (e.g., a sequence of words).
- It is strictly more powerful than a Convolutional Neural Network (CNN).

### 9. In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent?

<img width="551" alt="Screenshot 2021-05-16 at 20 23 20" src="https://user-images.githubusercontent.com/38349049/118408171-995ed780-b684-11eb-904e-9f4e4f92d3a9.png"><br>

- :white_check_mark: x-axis is the amount of data
- :white_check_mark: y-axis (vertical axis) is the performance of the algorithm.

### 10. Assuming the trends described in the previous question's figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.)

- Decreasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.
- :white_check_mark: Increasing the size of a neural network generally does not hurt an algorithm’s performance, and it
  may help significantly.
- :white_check_mark: Increasing the training set size generally does not hurt an algorithm’s performance, and it may
  help significantly.
- Decreasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help
  significantly.

